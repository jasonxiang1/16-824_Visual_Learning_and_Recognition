{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Even deeper! Resnet18 for PASCAL classification (15 pts)\n",
    "\n",
    "Hopefully we all got much better accuracy with the deeper model! Since 2012, much deeper architectures have been proposed. [ResNet](https://arxiv.org/abs/1512.03385) is one of the popular ones. In this task, we attempt to further improve the performance with the “very deep” ResNet-18 architecture.\n",
    "\n",
    "\n",
    "## 3.1 Build ResNet-18 (1 pts)\n",
    "Write a network modules for the Resnet-18 architecture (refer to the original paper). You can use `torchvision.models` for this section, so it should be very easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import trainer\n",
    "from utils import ARGS\n",
    "from simple_cnn import SimpleCNN\n",
    "from voc_dataset import VOCDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Add Tensorboard Summaries (6 pts)\n",
    "You should've already written tensorboard summary generation code into `trainer.py` from q1. However, you probably just added the most basic summary features. Please implement the more advanced summaries listed here:\n",
    "* training loss (should be done)\n",
    "* testing MAP curves (should be done)\n",
    "* learning rate\n",
    "* histogram of gradients\n",
    "\n",
    "## 3.3 Train and Test (8 pts)\n",
    "Use the same hyperparameter settings from Task 2, and train the model for 50 epochs. Report tensorboard screenshots for *all* of the summaries listed above (for image summaries show screenshots at $n \\geq 3$ iterations)\n",
    "\n",
    "**REMEMBER TO SAVE A MODEL AT THE END OF TRAINING**\n",
    "\n",
    "The figure shown below display the training loss, training mAP, learning rate, and histogram of gradients as displayed through TensorBoard.\n",
    "\n",
    "![title](img/q3_img02_loss.JPG)\n",
    "\n",
    "![title](img/q3_img03_map.JPG)\n",
    "\n",
    "![title](img/q3_img01_learningrate.JPG)\n",
    "\n",
    "The following 4 images below showcase histogram of gradients for the first two layers:\n",
    "\n",
    "![title](img/resnet_histogramofgradients_q3/01.JPG)\n",
    "![title](img/resnet_histogramofgradients_q3/02.JPG)\n",
    "![title](img/resnet_histogramofgradients_q3/03.JPG)\n",
    "![title](img/resnet_histogramofgradients_q3/04.JPG)\n",
    "\n",
    "The following 4 images below showcase histogram of gradients for the middle two layers:\n",
    "\n",
    "![title](img/resnet_histogramofgradients_q3/35.JPG)\n",
    "![title](img/resnet_histogramofgradients_q3/36.JPG)\n",
    "![title](img/resnet_histogramofgradients_q3/37.JPG)\n",
    "![title](img/resnet_histogramofgradients_q3/38.JPG)\n",
    "\n",
    "The following 4 images below showcase histogram of gradients for the last two layers:\n",
    "\n",
    "![title](img/resnet_histogramofgradients_q3/61.JPG)\n",
    "![title](img/resnet_histogramofgradients_q3/62.JPG)\n",
    "![title](img/resnet_histogramofgradients_q3/63.JPG)\n",
    "![title](img/resnet_histogramofgradients_q3/64.JPG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ARGS(batch_size = 32, epochs=50, lr = 0.0001)\n",
    "args.gamma = 0.3\n",
    "modelres = models.resnet18(pretrained=False)\n",
    "model= nn.Sequential(modelres,nn.Linear(1000,20,bias=True))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=args.gamma)\n",
    "if __name__ == '__main__':\n",
    "    test_ap, test_map = trainer.train(args, model, optimizer, scheduler)\n",
    "    print('test map:', test_map)"
   ]
  },
  {
   "source": [
    "Appendex A: Training Epoch Information\n",
    "\n",
    "The following log shown below displays the batch iteration, loss, and mAP calculation at various points during the training process:\n",
    "\n",
    "Train Epoch: 0 [0 (0%)]\tLoss: 0.720259 | mAP: 0.073370 <br/>\n",
    "Train Epoch: 0 [100 (64%)]\tLoss: 0.215044 | mAP: 0.189455 <br/>\n",
    "Train Epoch: 1 [200 (27%)]\tLoss: 0.216712 | mAP: 0.226419 <br/>\n",
    "Train Epoch: 1 [300 (91%)]\tLoss: 0.208612 | mAP: 0.234548 <br/>\n",
    "Train Epoch: 2 [400 (55%)]\tLoss: 0.181140 | mAP: 0.279389 <br/>\n",
    "Train Epoch: 3 [500 (18%)]\tLoss: 0.202289 | mAP: 0.311690 <br/>\n",
    "Train Epoch: 3 [600 (82%)]\tLoss: 0.197823 | mAP: 0.323791 <br/>\n",
    "Train Epoch: 4 [700 (46%)]\tLoss: 0.184834 | mAP: 0.330959 <br/>\n",
    "Train Epoch: 5 [800 (10%)]\tLoss: 0.185951 | mAP: 0.346840 <br/>\n",
    "Train Epoch: 5 [900 (73%)]\tLoss: 0.165875 | mAP: 0.347633 <br/>\n",
    "Train Epoch: 6 [1000 (37%)]\tLoss: 0.165044 | mAP: 0.372633 <br/>\n",
    "Train Epoch: 7 [1100 (1%)]\tLoss: 0.156291 | mAP: 0.397830 <br/>\n",
    "Train Epoch: 7 [1200 (64%)]\tLoss: 0.149558 | mAP: 0.405792 <br/>\n",
    "Train Epoch: 8 [1300 (28%)]\tLoss: 0.155647 | mAP: 0.409463 <br/>\n",
    "Train Epoch: 8 [1400 (92%)]\tLoss: 0.146398 | mAP: 0.414038 <br/>\n",
    "Train Epoch: 9 [1500 (55%)]\tLoss: 0.153696 | mAP: 0.396369 <br/>\n",
    "Train Epoch: 10 [1600 (19%)]\tLoss: 0.137831 | mAP: 0.410398 <br/>\n",
    "Train Epoch: 10 [1700 (83%)]\tLoss: 0.167851 | mAP: 0.430070 <br/>\n",
    "Train Epoch: 11 [1800 (46%)]\tLoss: 0.130406 | mAP: 0.445091 <br/>\n",
    "Train Epoch: 12 [1900 (10%)]\tLoss: 0.152793 | mAP: 0.440219 <br/>\n",
    "Train Epoch: 12 [2000 (74%)]\tLoss: 0.140506 | mAP: 0.434110 <br/>\n",
    "Train Epoch: 13 [2100 (38%)]\tLoss: 0.140575 | mAP: 0.455477 <br/>\n",
    "Train Epoch: 14 [2200 (1%)]\tLoss: 0.118644 | mAP: 0.456327 <br/>\n",
    "Train Epoch: 14 [2300 (65%)]\tLoss: 0.138294 | mAP: 0.445303 <br/>\n",
    "Train Epoch: 15 [2400 (29%)]\tLoss: 0.103462 | mAP: 0.494774 <br/>\n",
    "Train Epoch: 15 [2500 (92%)]\tLoss: 0.114315 | mAP: 0.497309 <br/>\n",
    "Train Epoch: 16 [2600 (56%)]\tLoss: 0.105307 | mAP: 0.506209 <br/>\n",
    "Train Epoch: 17 [2700 (20%)]\tLoss: 0.126236 | mAP: 0.505680 <br/>\n",
    "Train Epoch: 17 [2800 (83%)]\tLoss: 0.111384 | mAP: 0.506179 <br/>\n",
    "Train Epoch: 18 [2900 (47%)]\tLoss: 0.089492 | mAP: 0.491313 <br/>\n",
    "Train Epoch: 19 [3000 (11%)]\tLoss: 0.129798 | mAP: 0.506378 <br/>\n",
    "Train Epoch: 19 [3100 (75%)]\tLoss: 0.124896 | mAP: 0.506707 <br/>\n",
    "Train Epoch: 20 [3200 (38%)]\tLoss: 0.097039 | mAP: 0.509701 <br/>\n",
    "Train Epoch: 21 [3300 (2%)]\tLoss: 0.069134 | mAP: 0.498853 <br/>\n",
    "Train Epoch: 21 [3400 (66%)]\tLoss: 0.109458 | mAP: 0.506017 <br/>\n",
    "Train Epoch: 22 [3500 (29%)]\tLoss: 0.070703 | mAP: 0.500423 <br/>\n",
    "Train Epoch: 22 [3600 (93%)]\tLoss: 0.074366 | mAP: 0.504471 <br/>\n",
    "Train Epoch: 23 [3700 (57%)]\tLoss: 0.092572 | mAP: 0.503754 <br/>\n",
    "Train Epoch: 24 [3800 (20%)]\tLoss: 0.100229 | mAP: 0.504639 <br/>\n",
    "Train Epoch: 24 [3900 (84%)]\tLoss: 0.089912 | mAP: 0.508232 <br/>\n",
    "Train Epoch: 25 [4000 (48%)]\tLoss: 0.083635 | mAP: 0.502918 <br/>\n",
    "Train Epoch: 26 [4100 (11%)]\tLoss: 0.083434 | mAP: 0.507066 <br/>\n",
    "Train Epoch: 26 [4200 (75%)]\tLoss: 0.070950 | mAP: 0.511265 <br/>\n",
    "Train Epoch: 27 [4300 (39%)]\tLoss: 0.073017 | mAP: 0.503205 <br/>\n",
    "Train Epoch: 28 [4400 (3%)]\tLoss: 0.074057 | mAP: 0.509178 <br/>\n",
    "Train Epoch: 28 [4500 (66%)]\tLoss: 0.089060 | mAP: 0.508615 <br/>\n",
    "Train Epoch: 29 [4600 (30%)]\tLoss: 0.081091 | mAP: 0.508820 <br/>\n",
    "Train Epoch: 29 [4700 (94%)]\tLoss: 0.082942 | mAP: 0.511548 <br/>\n",
    "Train Epoch: 30 [4800 (57%)]\tLoss: 0.063460 | mAP: 0.518176 <br/>\n",
    "Train Epoch: 31 [4900 (21%)]\tLoss: 0.073838 | mAP: 0.515324 <br/>\n",
    "Train Epoch: 31 [5000 (85%)]\tLoss: 0.062788 | mAP: 0.514318 <br/>\n",
    "Train Epoch: 32 [5100 (48%)]\tLoss: 0.067382 | mAP: 0.518381 <br/>\n",
    "Train Epoch: 33 [5200 (12%)]\tLoss: 0.063792 | mAP: 0.510551 <br/>\n",
    "Train Epoch: 33 [5300 (76%)]\tLoss: 0.058528 | mAP: 0.511280 <br/>\n",
    "Train Epoch: 34 [5400 (39%)]\tLoss: 0.080845 | mAP: 0.512993 <br/>\n",
    "Train Epoch: 35 [5500 (3%)]\tLoss: 0.071587 | mAP: 0.515748 <br/>\n",
    "Train Epoch: 35 [5600 (67%)]\tLoss: 0.066521 | mAP: 0.516310 <br/>\n",
    "Train Epoch: 36 [5700 (31%)]\tLoss: 0.038676 | mAP: 0.513884 <br/>\n",
    "Train Epoch: 36 [5800 (94%)]\tLoss: 0.109776 | mAP: 0.514998 <br/>\n",
    "Train Epoch: 37 [5900 (58%)]\tLoss: 0.056390 | mAP: 0.509673 <br/>\n",
    "Train Epoch: 38 [6000 (22%)]\tLoss: 0.057494 | mAP: 0.517916 <br/>\n",
    "Train Epoch: 38 [6100 (85%)]\tLoss: 0.048027 | mAP: 0.514223 <br/>\n",
    "Train Epoch: 39 [6200 (49%)]\tLoss: 0.062421 | mAP: 0.510778 <br/>\n",
    "Train Epoch: 40 [6300 (13%)]\tLoss: 0.064487 | mAP: 0.508616 <br/>\n",
    "Train Epoch: 40 [6400 (76%)]\tLoss: 0.060632 | mAP: 0.512278 <br/>\n",
    "Train Epoch: 41 [6500 (40%)]\tLoss: 0.076664 | mAP: 0.509637 <br/>\n",
    "Train Epoch: 42 [6600 (4%)]\tLoss: 0.053598 | mAP: 0.513044 <br/>\n",
    "Train Epoch: 42 [6700 (68%)]\tLoss: 0.054771 | mAP: 0.512391 <br/>\n",
    "Train Epoch: 43 [6800 (31%)]\tLoss: 0.044373 | mAP: 0.511334 <br/>\n",
    "Train Epoch: 43 [6900 (95%)]\tLoss: 0.071721 | mAP: 0.509659 <br/>\n",
    "Train Epoch: 44 [7000 (59%)]\tLoss: 0.068732 | mAP: 0.509379 <br/>\n",
    "Train Epoch: 45 [7100 (22%)]\tLoss: 0.108448 | mAP: 0.510053 <br/>\n",
    "Train Epoch: 45 [7200 (86%)]\tLoss: 0.069663 | mAP: 0.512628 <br/>\n",
    "Train Epoch: 46 [7300 (50%)]\tLoss: 0.054835 | mAP: 0.510637 <br/>\n",
    "Train Epoch: 47 [7400 (13%)]\tLoss: 0.069490 | mAP: 0.511583 <br/>\n",
    "Train Epoch: 47 [7500 (77%)]\tLoss: 0.059993 | mAP: 0.508102 <br/>\n",
    "Train Epoch: 48 [7600 (41%)]\tLoss: 0.058032 | mAP: 0.512003 <br/>\n",
    "Train Epoch: 49 [7700 (4%)]\tLoss: 0.056961 | mAP: 0.513160 <br/>\n",
    "Train Epoch: 49 [7800 (68%)]\tLoss: 0.047862 | mAP: 0.509374 <br/>\n",
    "test map: 0.5124332392148173"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}